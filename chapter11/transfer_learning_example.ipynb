{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n",
      "(60000,)\n",
      "uint8\n",
      "(10000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)\n",
    "print(y_train_full.shape)\n",
    "print(y_train_full.dtype)\n",
    "print(X_test.shape)\n",
    "print(X_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size A = 48000\n",
      "size B = 12000\n"
     ]
    }
   ],
   "source": [
    "A_indices = np.where(y_train_full<8)\n",
    "B_indices = np.where(y_train_full>=8)\n",
    "\n",
    "X_train_full_A = X_train_full[A_indices]\n",
    "X_train_full_B = X_train_full[B_indices]\n",
    "\n",
    "y_train_full_A = y_train_full[A_indices]\n",
    "y_train_full_B = y_train_full[B_indices]\n",
    "\n",
    "print(\"size A = {}\".format(len(y_train_full_A)))\n",
    "print(\"size B = {}\".format(len(y_train_full_B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5,\n",
      "       7, 9, 1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7,\n",
      "       6, 7, 2, 1, 2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5, 1, 1,\n",
      "       2, 3, 9, 8, 7, 0, 2, 6, 2, 3, 1, 2, 8, 4, 1, 8, 5, 9, 5, 0, 3, 2,\n",
      "       0, 6, 5, 3, 6, 7, 1, 8, 0, 1, 4, 2], dtype=uint8)\n",
      "'len y_test_A = 8000'\n",
      "'len y_test_B = 2000'\n",
      "array([2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 0, 2, 5, 7, 1,\n",
      "       4, 6, 0, 3, 3, 3, 0, 7, 5, 7, 6, 1, 3, 7, 6, 7, 2, 1, 2, 2, 4, 4,\n",
      "       5, 2, 2, 4, 0, 7, 7, 5, 1, 1, 2, 3, 7, 0, 2, 6, 2, 3, 1, 2, 4, 1,\n",
      "       5, 5, 0, 3, 2, 0, 6, 5, 3, 6, 7, 1, 0, 1, 4, 2, 3, 6, 7, 2, 7, 5,\n",
      "       4, 2, 5, 7, 0, 5, 2, 6, 7, 0, 0, 3], dtype=uint8)\n",
      "array([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "       1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0], dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "A_indices_test = np.where(y_test<8)\n",
    "B_indices_test = np.where(y_test>=8)\n",
    "\n",
    "y_test_A = y_test[A_indices_test]\n",
    "y_test_B = y_test[B_indices_test]\n",
    "y_test_B = y_test_B - 8\n",
    "\n",
    "X_test_A = X_test[A_indices_test] / 255.0\n",
    "X_test_B = X_test[B_indices_test] / 255.0\n",
    "\n",
    "pprint(y_test[:100])\n",
    "pprint(\"len y_test_A = {}\".format(len(y_test_A)))\n",
    "pprint(\"len y_test_B = {}\".format(len(y_test_B)))\n",
    "pprint(y_test_A[:100])\n",
    "pprint(y_test_B[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "       0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "       0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1], dtype=uint8)\n",
      "array([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "       0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1], dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "X_valid_A, X_train_A = X_train_full_A[:4000] / 255.0, X_train_full_A[4000:] / 255.0\n",
    "y_valid_A, y_train_A = y_train_full_A[:4000], y_train_full_A[4000:]\n",
    "\n",
    "X_valid_B, X_train_B = X_train_full_B[:1000] / 255.0, X_train_full_B[1000:] / 255.0\n",
    "y_valid_B, y_train_B = y_train_full_B[:1000], y_train_full_B[1000:]\n",
    "\n",
    "y_train_B = y_train_B - 8\n",
    "y_valid_B = y_valid_B - 8\n",
    "\n",
    "pprint(y_train_B[:100])\n",
    "pprint(y_valid_B[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_A = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\"]\n",
    "class_names_B = [\"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 266,408\n",
      "Trainable params: 266,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model_A.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model_A.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "print(model_A.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44000 samples, validate on 4000 samples\n",
      "Epoch 1/30\n",
      "44000/44000 [==============================] - 3s 63us/sample - loss: 0.5055 - accuracy: 0.8141 - val_loss: 0.4157 - val_accuracy: 0.8462\n",
      "Epoch 2/30\n",
      "44000/44000 [==============================] - 2s 52us/sample - loss: 0.3922 - accuracy: 0.8511 - val_loss: 0.3710 - val_accuracy: 0.8640\n",
      "Epoch 3/30\n",
      "44000/44000 [==============================] - 2s 53us/sample - loss: 0.3534 - accuracy: 0.8657 - val_loss: 0.3687 - val_accuracy: 0.8615\n",
      "Epoch 4/30\n",
      "44000/44000 [==============================] - 2s 53us/sample - loss: 0.3323 - accuracy: 0.8740 - val_loss: 0.3329 - val_accuracy: 0.8770\n",
      "Epoch 5/30\n",
      "44000/44000 [==============================] - 2s 55us/sample - loss: 0.3156 - accuracy: 0.8814 - val_loss: 0.3473 - val_accuracy: 0.8662\n",
      "Epoch 6/30\n",
      "44000/44000 [==============================] - 2s 55us/sample - loss: 0.3012 - accuracy: 0.8855 - val_loss: 0.3532 - val_accuracy: 0.8698\n",
      "Epoch 7/30\n",
      "44000/44000 [==============================] - 2s 56us/sample - loss: 0.2868 - accuracy: 0.8918 - val_loss: 0.3225 - val_accuracy: 0.8813\n",
      "Epoch 8/30\n",
      "44000/44000 [==============================] - 2s 52us/sample - loss: 0.2739 - accuracy: 0.8965 - val_loss: 0.3371 - val_accuracy: 0.8760\n",
      "Epoch 9/30\n",
      "44000/44000 [==============================] - 2s 52us/sample - loss: 0.2644 - accuracy: 0.9000 - val_loss: 0.3361 - val_accuracy: 0.8848\n",
      "Epoch 10/30\n",
      "44000/44000 [==============================] - 2s 52us/sample - loss: 0.2554 - accuracy: 0.9023 - val_loss: 0.3231 - val_accuracy: 0.8827\n",
      "Epoch 11/30\n",
      "44000/44000 [==============================] - 2s 53us/sample - loss: 0.2434 - accuracy: 0.9067 - val_loss: 0.3802 - val_accuracy: 0.8670\n",
      "Epoch 12/30\n",
      "44000/44000 [==============================] - 2s 53us/sample - loss: 0.2380 - accuracy: 0.9097 - val_loss: 0.3446 - val_accuracy: 0.8820\n",
      "Epoch 13/30\n",
      "44000/44000 [==============================] - 2s 53us/sample - loss: 0.2274 - accuracy: 0.9118 - val_loss: 0.3419 - val_accuracy: 0.8820\n",
      "Epoch 14/30\n",
      "44000/44000 [==============================] - 2s 53us/sample - loss: 0.2173 - accuracy: 0.9168 - val_loss: 0.3331 - val_accuracy: 0.8785\n",
      "Epoch 15/30\n",
      "44000/44000 [==============================] - 2s 53us/sample - loss: 0.2109 - accuracy: 0.9183 - val_loss: 0.3420 - val_accuracy: 0.8790\n",
      "Epoch 16/30\n",
      "44000/44000 [==============================] - 3s 59us/sample - loss: 0.2061 - accuracy: 0.9205 - val_loss: 0.3349 - val_accuracy: 0.8885\n",
      "Epoch 17/30\n",
      "44000/44000 [==============================] - 2s 55us/sample - loss: 0.1990 - accuracy: 0.9228 - val_loss: 0.3315 - val_accuracy: 0.8867\n",
      "Epoch 18/30\n",
      "44000/44000 [==============================] - 2s 55us/sample - loss: 0.1899 - accuracy: 0.9270 - val_loss: 0.3537 - val_accuracy: 0.8898\n",
      "Epoch 19/30\n",
      "44000/44000 [==============================] - 2s 57us/sample - loss: 0.1882 - accuracy: 0.9278 - val_loss: 0.3655 - val_accuracy: 0.8855\n",
      "Epoch 20/30\n",
      "44000/44000 [==============================] - 2s 56us/sample - loss: 0.1834 - accuracy: 0.9308 - val_loss: 0.4043 - val_accuracy: 0.8823\n",
      "Epoch 21/30\n",
      "44000/44000 [==============================] - 3s 57us/sample - loss: 0.1765 - accuracy: 0.9317 - val_loss: 0.3880 - val_accuracy: 0.8780\n",
      "Epoch 22/30\n",
      "44000/44000 [==============================] - 2s 57us/sample - loss: 0.1714 - accuracy: 0.9332 - val_loss: 0.3852 - val_accuracy: 0.8860\n",
      "Epoch 23/30\n",
      "44000/44000 [==============================] - 3s 57us/sample - loss: 0.1657 - accuracy: 0.9367 - val_loss: 0.3629 - val_accuracy: 0.8940\n",
      "Epoch 24/30\n",
      "44000/44000 [==============================] - 2s 56us/sample - loss: 0.1615 - accuracy: 0.9375 - val_loss: 0.4033 - val_accuracy: 0.8865\n",
      "Epoch 25/30\n",
      "44000/44000 [==============================] - 2s 57us/sample - loss: 0.1570 - accuracy: 0.9402 - val_loss: 0.3869 - val_accuracy: 0.8830\n",
      "Epoch 26/30\n",
      "44000/44000 [==============================] - 3s 58us/sample - loss: 0.1560 - accuracy: 0.9393 - val_loss: 0.4269 - val_accuracy: 0.8848\n",
      "Epoch 27/30\n",
      "44000/44000 [==============================] - 3s 57us/sample - loss: 0.1476 - accuracy: 0.9411 - val_loss: 0.4048 - val_accuracy: 0.8867\n",
      "Epoch 28/30\n",
      "44000/44000 [==============================] - 3s 59us/sample - loss: 0.1463 - accuracy: 0.9443 - val_loss: 0.4042 - val_accuracy: 0.8890\n",
      "Epoch 29/30\n",
      "44000/44000 [==============================] - 3s 58us/sample - loss: 0.1425 - accuracy: 0.9450 - val_loss: 0.4255 - val_accuracy: 0.8873\n",
      "Epoch 30/30\n",
      "44000/44000 [==============================] - 3s 60us/sample - loss: 0.1372 - accuracy: 0.9468 - val_loss: 0.4246 - val_accuracy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=30, validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 23us/sample - loss: 2.4109 - accuracy: 0.8491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.410911674499511, 0.849125]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.evaluate(X_test_A, y_test_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11000 samples, validate on 1000 samples\n",
      "Epoch 1/4\n",
      "11000/11000 [==============================] - 1s 48us/sample - loss: 0.7981 - accuracy: 0.8061 - val_loss: 0.0609 - val_accuracy: 0.9880\n",
      "Epoch 2/4\n",
      "11000/11000 [==============================] - 0s 38us/sample - loss: 0.0471 - accuracy: 0.9892 - val_loss: 0.0240 - val_accuracy: 0.9950\n",
      "Epoch 3/4\n",
      "11000/11000 [==============================] - 0s 38us/sample - loss: 0.0232 - accuracy: 0.9943 - val_loss: 0.0153 - val_accuracy: 0.9970\n",
      "Epoch 4/4\n",
      "11000/11000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9959 - val_loss: 0.0116 - val_accuracy: 0.9970\n",
      "Train on 11000 samples, validate on 1000 samples\n",
      "Epoch 1/16\n",
      "11000/11000 [==============================] - 1s 80us/sample - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0052 - val_accuracy: 0.9980\n",
      "Epoch 2/16\n",
      "11000/11000 [==============================] - 1s 54us/sample - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0054 - val_accuracy: 0.9990\n",
      "Epoch 3/16\n",
      "11000/11000 [==============================] - 1s 57us/sample - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0055 - val_accuracy: 0.9990\n",
      "Epoch 4/16\n",
      "11000/11000 [==============================] - 1s 54us/sample - loss: 9.5548e-04 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9990\n",
      "Epoch 5/16\n",
      "11000/11000 [==============================] - 1s 54us/sample - loss: 6.9394e-04 - accuracy: 0.9999 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
      "Epoch 6/16\n",
      "11000/11000 [==============================] - 1s 53us/sample - loss: 4.8789e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9990\n",
      "Epoch 7/16\n",
      "11000/11000 [==============================] - 1s 53us/sample - loss: 3.7821e-04 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 0.9990\n",
      "Epoch 8/16\n",
      "11000/11000 [==============================] - 1s 62us/sample - loss: 2.6649e-04 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9990\n",
      "Epoch 9/16\n",
      "11000/11000 [==============================] - 1s 56us/sample - loss: 2.0529e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
      "Epoch 10/16\n",
      "11000/11000 [==============================] - 1s 55us/sample - loss: 1.6442e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
      "Epoch 11/16\n",
      "11000/11000 [==============================] - 1s 55us/sample - loss: 1.3510e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9990\n",
      "Epoch 12/16\n",
      "11000/11000 [==============================] - 1s 57us/sample - loss: 9.9469e-05 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9990\n",
      "Epoch 13/16\n",
      "11000/11000 [==============================] - 1s 57us/sample - loss: 8.5136e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9990\n",
      "Epoch 14/16\n",
      "11000/11000 [==============================] - 1s 56us/sample - loss: 6.5073e-05 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9990\n",
      "Epoch 15/16\n",
      "11000/11000 [==============================] - 1s 60us/sample - loss: 5.6420e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9990\n",
      "Epoch 16/16\n",
      "11000/11000 [==============================] - 1s 64us/sample - loss: 4.5707e-05 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9990\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "       0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1], dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "pprint(y_valid_B[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
