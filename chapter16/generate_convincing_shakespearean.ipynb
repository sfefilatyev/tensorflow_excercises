{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one of the recent language models (e.g., GPT) to generate more convincing Shakespearean text.\n",
    "# The simplest way to use recent language models is to use the excellent transformers library, \n",
    "# open sourced by Hugging Face. It provides many modern neural net architectures \n",
    "# (including BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet and more) for Natural Language Processing (NLP), \n",
    "# including many pretrained models. It relies on either TensorFlow or PyTorch. Best of all: \n",
    "# it's amazingly simple to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.experimental.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "\n",
    "print(keras.__version__)\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load a pretrained model. In this example, we will use OpenAI's GPT model, \n",
    "# with an additional Language Model on top (just a linear layer with weights tied to the input embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=656.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466312920.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFOpenAIGPTLMHeadModel.\n",
      "\n",
      "All the layers of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "\n",
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also need a specialized tokenizer for this model. This one will try to use the spaCy and ftfy libraries \n",
    "# if they are installed, or else it will fall back to BERT's BasicTokenizer followed by Byte-Pair Encoding \n",
    "# (which should be fine for most use cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=815973.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=458495.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the prmompt text\n",
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text,\n",
    "                                  add_special_tokens=False,\n",
    "                                  return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   488,   481,  7795,   498,  7144,   557,   239, 40477,\n",
       "          674,   481, 10891,   260,  1092,  2923,  2034,   481,  1705,\n",
       "         1294,  2075,   498,   481,  4204,   240,   481,  2998,  2360,\n",
       "          525,  6851,  3415,   617,   987, 13703,  2034,   481,  2761,\n",
       "          260,  5693,   240,   481,  5616],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   544,   808,   246,  1938,   638,   841,   617,   481,\n",
       "        20090,   239,   568,   669,   481,  1621,   544,  2923,   606,\n",
       "          994,   727,   551,   498,   481, 16187,   781,   775,  4000,\n",
       "          485,  1529,   507,   504,   481,  4802,   239,   481, 29401,\n",
       "         1549,   544,  1949,   239,   244],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   544, 25898,   702,  1272, 19175,   240,   488,\n",
       "          557,  1389,   544,   595,  1252,   500,   622,  1463,   240,\n",
       "          808,   562,   481,   618,  1146,   239,   507,   544,   603,\n",
       "          525,   481,  4160,   618,   498,   481, 12191,   886,   246,\n",
       "         4648,   485,   481, 12191,   240],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   498, 12679,   240,   488,   616,   618,   488,  1288,\n",
       "        14661,   240,   763,  1787,   485, 12413,   481,  2021,   240,\n",
       "         4512,   246,   783,  1276,   806,  2676,   488,  2676,   635,\n",
       "          580,   498,  1529,   485,  1709,   239,   244, 40477,  2018,\n",
       "         4283,  1223,   481,   844,   239],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   617,   481,  2233,   498,   486,   478, 17102,   702,\n",
       "          481,  5751,   623,   979,  2012,   766,   498,  1070, 40133,\n",
       "          239,   488,   481,   929,  1582,   566,   749, 40477,   256,\n",
       "          525,   535,   599, 31387, 12644,  2443,   240,   249,   825,\n",
       "          240,   256, 40133,  1404,   603]], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the model to generate text after the prompt. We will generate 5 different sentences, \n",
    "# each starting with the prompt text, followed by 40 additional tokens. \n",
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle and the arch of pelas. \n",
      " then the jewel - light settled upon the cold dark edge of the plain, the pale lights that shone forth from its cliffs upon the sea - shore, the brilliant\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle is only a short way away from the mainland. but when the matter is settled we should get out of the isle before any attempt to use it on the tower. the driftwood wall is clear. \"\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle, is revered by many peoples, and as such is not used in our family, only for the king himself. it is said that the ancient king of the seas got a duty to the seas,\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle of insanity, and this king and these mages, who continued to worship the child, created a new world where magic and magic could be of use to others. \" \n",
      " silence swept across the room.\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle from the middle of erindell by the royal subibulation of draegon. and the first real one... \n",
      "'that's what maekar wants, i think,'egon finally said\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# decode the generated sequences and print them\n",
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
