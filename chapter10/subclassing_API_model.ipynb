{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sfefilatyev/projects/tensorflow_exercises/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method WideAndDeepModel.call of <__main__.WideAndDeepModel object at 0x7ff2b7417438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WideAndDeepModel.call of <__main__.WideAndDeepModel object at 0x7ff2b7417438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method WideAndDeepModel.call of <__main__.WideAndDeepModel object at 0x7ff2b7417438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method WideAndDeepModel.call of <__main__.WideAndDeepModel object at 0x7ff2b7417438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 2.1375 - output_1_loss: 1.8033 - output_2_loss: 5.1436 - val_loss: 1.1877 - val_output_1_loss: 0.8009 - val_output_2_loss: 4.6654\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.9789 - output_1_loss: 0.7418 - output_2_loss: 3.1132 - val_loss: 0.8693 - val_output_1_loss: 0.6375 - val_output_2_loss: 2.9530\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.8056 - output_1_loss: 0.6526 - output_2_loss: 2.1812 - val_loss: 0.7456 - val_output_1_loss: 0.5854 - val_output_2_loss: 2.1877\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.7277 - output_1_loss: 0.6132 - output_2_loss: 1.7570 - val_loss: 0.6810 - val_output_1_loss: 0.5547 - val_output_2_loss: 1.8165\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.6833 - output_1_loss: 0.5869 - output_2_loss: 1.5525 - val_loss: 0.6426 - val_output_1_loss: 0.5338 - val_output_2_loss: 1.6201\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.6534 - output_1_loss: 0.5659 - output_2_loss: 1.4400 - val_loss: 0.6150 - val_output_1_loss: 0.5158 - val_output_2_loss: 1.5077\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.6315 - output_1_loss: 0.5491 - output_2_loss: 1.3711 - val_loss: 0.5940 - val_output_1_loss: 0.5001 - val_output_2_loss: 1.4384\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.6134 - output_1_loss: 0.5343 - output_2_loss: 1.3239 - val_loss: 0.5779 - val_output_1_loss: 0.4889 - val_output_2_loss: 1.3778\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5989 - output_1_loss: 0.5222 - output_2_loss: 1.2893 - val_loss: 0.5631 - val_output_1_loss: 0.4775 - val_output_2_loss: 1.3324\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.5863 - output_1_loss: 0.5114 - output_2_loss: 1.2590 - val_loss: 0.5512 - val_output_1_loss: 0.4682 - val_output_2_loss: 1.2986\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.5763 - output_1_loss: 0.5032 - output_2_loss: 1.2341 - val_loss: 0.5424 - val_output_1_loss: 0.4628 - val_output_2_loss: 1.2608\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5664 - output_1_loss: 0.4949 - output_2_loss: 1.2124 - val_loss: 0.5320 - val_output_1_loss: 0.4543 - val_output_2_loss: 1.2322\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.5575 - output_1_loss: 0.4870 - output_2_loss: 1.1907 - val_loss: 0.5239 - val_output_1_loss: 0.4481 - val_output_2_loss: 1.2051\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.5504 - output_1_loss: 0.4812 - output_2_loss: 1.1711 - val_loss: 0.5169 - val_output_1_loss: 0.4431 - val_output_2_loss: 1.1802\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5431 - output_1_loss: 0.4752 - output_2_loss: 1.1527 - val_loss: 0.5096 - val_output_1_loss: 0.4376 - val_output_2_loss: 1.1573\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.5368 - output_1_loss: 0.4702 - output_2_loss: 1.1356 - val_loss: 0.5045 - val_output_1_loss: 0.4348 - val_output_2_loss: 1.1322\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5312 - output_1_loss: 0.4659 - output_2_loss: 1.1188 - val_loss: 0.4998 - val_output_1_loss: 0.4316 - val_output_2_loss: 1.1124\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5259 - output_1_loss: 0.4621 - output_2_loss: 1.1014 - val_loss: 0.4954 - val_output_1_loss: 0.4288 - val_output_2_loss: 1.0947\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5208 - output_1_loss: 0.4580 - output_2_loss: 1.0867 - val_loss: 0.4901 - val_output_1_loss: 0.4253 - val_output_2_loss: 1.0733\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.5156 - output_1_loss: 0.4539 - output_2_loss: 1.0693 - val_loss: 0.4855 - val_output_1_loss: 0.4222 - val_output_2_loss: 1.0566\n",
      "5160/5160 [==============================] - 0s 18us/sample - loss: 0.5450 - output_1_loss: 0.4752 - output_2_loss: 1.1602\n",
      "0.544977605342865 0.47516266 1.1602175\n",
      "[[1.7946258]\n",
      " [1.1478887]\n",
      " [2.4087803]] [[2.3179743]\n",
      " [1.6350056]\n",
      " [1.7545929]]\n"
     ]
    }
   ],
   "source": [
    "# Subclassing-API model\n",
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")    \n",
    "    \n",
    "model5 = WideAndDeepModel()\n",
    "\n",
    "model5.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "main_output, aux_output = model5(inputs=[input_A, input_B])\n",
    "history = model5.fit((X_train_A, X_train_B), [y_train, y_train], epochs=20,\n",
    "                     validation_data=((X_valid_A, X_valid_B), [y_valid, y_valid]))\n",
    "total_loss, main_loss, aux_loss = model5.evaluate([X_test_A, X_test_B], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model5.predict([X_new_A, X_new_B])\n",
    "print(total_loss, main_loss, aux_loss)\n",
    "print(y_pred_main, y_pred_aux)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
